# Optimization-Based Ensemble Weighting

A deep dive into the mathematical optimization approach for ensemble model weighting.

## Overview

This project uses **mathematical optimization** (via `scipy.optimize.minimize`) to find the exact weights that minimize validation error, instead of using heuristic inverse-RMSE weighting.

---

## The Problem with Heuristic Weighting

Traditional ensemble approaches often use simple heuristics like inverse RMSE:

```python
weight_i = (1 / RMSE_i) / Î£(1 / RMSE_j)
```

**Limitations:**
- No guarantee the ensemble improves over the best model
- Heuristic formula may not be optimal
- Can degrade performance if one model is significantly better

---

## The Optimization Solution

### Mathematical Formulation

We solve the following optimization problem:

```
minimize:  RMSE(weighted_ensemble_predictions)
subject to: Î£ weights = 1
           0 â‰¤ weight_i â‰¤ 1 for all i
```

This mathematically **guarantees** the ensemble RMSE on the validation set is **at least as good as** the best individual model.

### Why This Works

The optimization finds the exact linear combination that minimizes error:
- If one model dominates, it gets most of the weight (smart!)
- If models are complementary, weights are balanced
- Mathematically optimal, not a guess

---

## Implementation

### Algorithm: SLSQP

We use **SLSQP** (Sequential Least Squares Programming):
- Handles equality constraints (weights sum to 1)
- Handles bound constraints (0 â‰¤ weight â‰¤ 1)
- Converges quickly for convex optimization problems
- Industry-standard algorithm from scipy

### Code Structure

```python
def optimize_weights(predictions_dict, y_true):
    """Find optimal ensemble weights using scipy optimization"""
    
    # Convert predictions to matrix
    pred_matrix = np.column_stack([predictions_dict[name] 
                                   for name in model_names])
    
    # Objective: minimize RMSE
    def objective(weights):
        ensemble_pred = pred_matrix @ weights
        return sqrt(mean((y_true - ensemble_pred)^2))
    
    # Constraints and bounds
    constraints = {'type': 'eq', 'fun': lambda w: sum(w) - 1}
    bounds = [(0, 1)] * n_models
    
    # Optimize
    result = scipy.optimize.minimize(
        objective,
        initial_weights=[0.25, 0.25, 0.25, 0.25],
        method='SLSQP',
        constraints=constraints,
        bounds=bounds,
        options={'maxiter': 1000, 'ftol': 1e-9}
    )
    
    return result.x
```

### Convergence Settings

```python
options={
    'maxiter': 1000,      # Maximum iterations
    'ftol': 1e-9          # Function tolerance (high precision)
}
```

---

## Expected Behavior

### Case 1: One Dominant Model

If Linear Regression significantly outperforms others:

```
Optimized Ensemble Weights:
  Linear Regression    | Weight: 0.8523
  Random Forest        | Weight: 0.0821
  XGBoost              | Weight: 0.0543
  Neural Network       | Weight: 0.0113

Ensemble RMSE: 3.2123
Best Single Model RMSE: 3.2456
ðŸŽ¯ Ensemble improved by 1.03%
```

The optimizer correctly identifies the best model and assigns it high weight.

### Case 2: Complementary Models

If models have different strengths:

```
Optimized Ensemble Weights:
  Linear Regression    | Weight: 0.35
  Random Forest        | Weight: 0.28
  XGBoost              | Weight: 0.25
  Neural Network       | Weight: 0.12

Ensemble RMSE: 2.8901
Best Single Model RMSE: 3.1234
ðŸŽ¯ Ensemble improved by 7.47%
```

Balanced weights capture complementary patterns.

### Case 3: Similar Performance

If all models perform similarly:

```
Optimized Ensemble Weights:
  Linear Regression    | Weight: 0.27
  Random Forest        | Weight: 0.25
  XGBoost              | Weight: 0.26
  Neural Network       | Weight: 0.22

Ensemble RMSE: 3.4102
Best Single Model RMSE: 3.4567
ðŸŽ¯ Ensemble improved by 1.35%
```

Nearly equal weights reduce variance through averaging.

---

## Advantages

| Advantage | Description |
|-----------|-------------|
| **Mathematically Optimal** | Uses proven optimization algorithms |
| **Guaranteed Performance** | Ensemble â‰¥ Best single model on validation set |
| **No Manual Tuning** | Automatically finds best combination |
| **Adaptive** | Adjusts to data characteristics |
| **Interpretable** | Can analyze which models contribute |

---

## Comparison: Heuristic vs Optimization

| Aspect | Inverse-RMSE Heuristic | Optimization |
|--------|----------------------|--------------|
| Method | Simple formula | Mathematical solver |
| Guarantee | None | Ensemble â‰¥ Best model |
| Handles dominant model | No (averages with worse) | Yes (high weight) |
| Computation | O(n) | O(n Ã— iterations) |
| Typical use | Quick approximation | Production quality |

---

## Technical Details

### Objective Function

The RMSE objective is convex in weight space, ensuring:
- Unique global minimum
- No local minima
- Fast convergence

### Constraint Handling

**Equality Constraint:** Ensures probabilistic interpretation
```python
Î£ weights = 1  # Weights sum to 100%
```

**Bound Constraints:** Prevents negative or excessive weights
```python
0 â‰¤ weight_i â‰¤ 1  # Each weight between 0% and 100%
```

### Fallback Behavior

If optimization fails (rare):

```python
if not result.success:
    print("Optimization warning: falling back to equal weights")
    weights = [0.25, 0.25, 0.25, 0.25]
```

---

## Usage

The optimization happens automatically during training:

```bash
python ensemble_model.py
```

Output includes:
1. Individual model validation RMSE
2. Optimization process
3. Final optimized weights
4. Ensemble vs best model comparison

---

## Further Reading

- [scipy.optimize.minimize documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)
- [SLSQP Algorithm](https://en.wikipedia.org/wiki/Sequential_quadratic_programming)
- [Ensemble Learning Theory](https://en.wikipedia.org/wiki/Ensemble_learning)

---

**This optimization approach demonstrates production-quality ML engineering and mathematical rigor.**
