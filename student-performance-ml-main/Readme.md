# ğŸ“ Student Performance Prediction - ML Ensemble Project

[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.12-orange.svg)](https://www.tensorflow.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28-red.svg)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

An advanced machine learning system that predicts student exam scores using an **intelligent ensemble of 4 ML algorithms** with **SHAP explainability** and an **interactive Streamlit dashboard**.

## ğŸŒŸ Key Differentiators

This project stands out through:

- **ğŸ¤– Multi-Model Ensemble**: Combines Linear Regression, Random Forest, XGBoost, and Neural Network with performance-based weights
- **ğŸ” Explainable AI**: SHAP (SHapley Additive exPlanations) for model interpretability
- **ğŸ“Š Interactive Dashboard**: Beautiful Streamlit web app for live predictions and visualizations
- **ğŸ“ˆ Comprehensive Analysis**: Deep performance evaluation with multiple metrics
- **ğŸ¯ Production-Ready**: Modular code structure with model persistence and configuration

---

## ğŸ“Š Model Performance

| Model | RMSE â†“ | MAE â†“ | RÂ² â†‘ | MAPE â†“ |
|-------|--------|-------|------|--------|
| **Weighted Ensemble** | TBD | TBD | TBD | TBD |
| XGBoost | TBD | TBD | TBD | TBD |
| Random Forest | TBD | TBD | TBD | TBD |
| Neural Network | TBD | TBD | TBD | TBD |
| Linear Regression | TBD | TBD | TBD | TBD |

*Run `python ensemble_model.py` to populate these metrics*

---

## ğŸš€ Quick Start

### 1. Installation

**Option A: Using Conda (Recommended)**
```bash
# Create environment from file
conda env create -f environment.yml

# Activate environment
conda activate student_perf
```

**Option B: Using pip**
```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Mac/Linux)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Train Models

```bash
# Train all 4 models and create weighted ensemble
python ensemble_model.py
```

This will:
- Train Linear Regression, Random Forest, XGBoost, and Neural Network
- Calculate optimal ensemble weights based on validation performance
- Save all models to `models/` directory
- Generate performance comparison visualizations in `results/`

### 3. Generate Explainability Analysis

```bash
# Create SHAP feature importance visualizations
python model_explainer.py
```

This generates:
- Feature importance rankings for each model
- SHAP summary plots showing feature impacts
- Waterfall plots for individual predictions

### 4. Launch Interactive Dashboard

```bash
# Start Streamlit web application
streamlit run streamlit_app.py
```

Then open your browser to `http://localhost:8501`

---

## ğŸ“ Project Structure

```
student-performance-ml/
â”‚
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ Raw/                          # Original dataset
â”‚   â””â”€â”€ Processed/                    # Train/test splits
â”‚       â”œâ”€â”€ X_train.csv
â”‚       â”œâ”€â”€ X_test.csv
â”‚       â”œâ”€â”€ y_train.csv
â”‚       â””â”€â”€ y_test.csv
â”‚
â”œâ”€â”€ models/                           # Saved trained models
â”‚   â”œâ”€â”€ linear_regression.joblib
â”‚   â”œâ”€â”€ random_forest.joblib
â”‚   â”œâ”€â”€ xgboost.joblib
â”‚   â”œâ”€â”€ neural_network.keras
â”‚   â””â”€â”€ ensemble_config.json          # Ensemble weights
â”‚
â”œâ”€â”€ results/                          # Outputs and visualizations
â”‚   â”œâ”€â”€ ensemble_performance.png
â”‚   â”œâ”€â”€ shap_feature_importance_comparison.png
â”‚   â””â”€â”€ model_comparison_results.png
â”‚
â”œâ”€â”€ Notebooks/                        # Jupyter notebooks for exploration
â”‚   â”œâ”€â”€ preprocessing.ipynb.ipynb
â”‚   â””â”€â”€ decision_tree_carrin.ipynb
â”‚
â”œâ”€â”€ src/                              # Source code modules
â”‚   â”œâ”€â”€ data_prep.py
â”‚   â”œâ”€â”€ neuralnetworkmethod.py
â”‚   â”œâ”€â”€ Random_Forest/
â”‚   â””â”€â”€ xgboost/
â”‚
â”œâ”€â”€ ensemble_model.py                 # Main ensemble training script
â”œâ”€â”€ model_explainer.py                # SHAP explainability module
â”œâ”€â”€ model_comparison.py               # Comprehensive model comparison
â”œâ”€â”€ streamlit_app.py                  # Interactive web dashboard
â”œâ”€â”€ utils.py                          # Shared utility functions
â”‚
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ environment.yml                   # Conda environment
â””â”€â”€ README.md                         # This file
```

---

## ğŸ¯ Features

### Machine Learning Models

1. **Linear Regression**
   - Fast baseline model
   - Interpretable coefficients
   - Shows linear relationships

2. **Random Forest**
   - 300 decision trees
   - Handles non-linearity well
   - Built-in feature importance

3. **XGBoost**
   - Gradient boosting algorithm
   - State-of-the-art tabular data performance
   - Optimized hyperparameters

4. **Neural Network**
   - 4 hidden layers (256â†’128â†’64â†’32)
   - Batch normalization & dropout
   - Early stopping for optimal training

5. **Weighted Ensemble**
   - Combines all 4 models
   - Weights based on validation RMSE
   - Typically achieves best performance

### Explainability (SHAP)

- **Feature Importance**: Identifies which factors most influence predictions
- **Summary Plots**: Visualizes feature impacts across all predictions
- **Waterfall Plots**: Explains individual student predictions
- **Model Comparison**: Shows how different models prioritize features differently

### Interactive Dashboard

- **ğŸ  Home**: Project overview and quick stats
- **ğŸ“Š Model Comparison**: Performance metrics and visualizations
- **ğŸ¯ Make Predictions**: Interactive form to predict scores for custom student profiles
- **ğŸ” Feature Importance**: SHAP visualizations and analysis
- **â„¹ï¸ About**: Detailed methodology and documentation

---

## ğŸ“š Dataset

- **Size**: 6,607 student records
- **Features**: 19 predictors including:
  - Academic: Hours studied, attendance, previous scores
  - Socio-economic: Family income, parental education, distance from home
  - Support: Tutoring sessions, teacher quality, internet access
  - Personal: Sleep hours, physical activity, learning disabilities
- **Target**: Exam score (0-100)

---

## ğŸ”¬ Methodology

### Ensemble Strategy

The weighted ensemble combines predictions using:

```
Ensemble Prediction = Î£ (weight_i Ã— prediction_i)
```

Where weights are calculated as:

```
weight_i = (1 / RMSE_i) / Î£(1 / RMSE_j)
```

This ensures better-performing models have more influence on the final prediction.

### Why Ensemble?

- **Reduced Overfitting**: Different models make different errors
- **Improved Accuracy**: Combines strengths of multiple approaches
- **Robustness**: Less sensitive to outliers and anomalies
- **Best Practice**: Used in winning solutions across ML competitions

---

## ğŸ“Š Usage Examples

### Making Predictions Programmatically

```python
from ensemble_model import StudentPerformanceEnsemble
import pandas as pd

# Load trained ensemble
ensemble = StudentPerformanceEnsemble()
ensemble.load_models()

# Prepare input (example values)
student_data = pd.DataFrame({
    'Hours_Studied': [25],
    'Attendance': [90],
    'Parental_Involvement': [1],  # Encoded value
    # ... other features
})

# Get predictions from all models
predictions = ensemble.predict_all(student_data)

print(f"Ensemble Prediction: {predictions['Ensemble'][0]:.2f}")
print(f"XGBoost Prediction: {predictions['XGBoost'][0]:.2f}")
```

### Generating SHAP Explanations

```python
from model_explainer import ModelExplainer

# Create explainer
explainer = ModelExplainer(ensemble, X_train, X_test, feature_names)
explainer.create_explainers()
explainer.calculate_shap_values()

# Get feature importance
importance = explainer.get_feature_importance('XGBoost')
print(importance.head(10))

# Explain single prediction
explanation = explainer.explain_single_prediction('XGBoost', instance_idx=0)
```

---

## ğŸ“ Educational Value

This project demonstrates:

- **Ensemble Learning**: Combining multiple models for better performance
- **Hyperparameter Tuning**: Optimizing model configurations
- **Model Explainability**: Using SHAP for interpretability
- **Deep Learning**: Building and training neural networks
- **Web Development**: Creating interactive ML dashboards
- **Software Engineering**: Modular, maintainable code structure
- **Data Science Workflow**: End-to-end ML pipeline

---

## ğŸ”§ Advanced Configuration

### Custom Ensemble Weights

Edit `ensemble_model.py` to manually set weights:

```python
self.weights = {
    'Linear Regression': 0.10,
    'Random Forest': 0.30,
    'XGBoost': 0.40,
    'Neural Network': 0.20
}
```

### Hyperparameter Tuning

Modify model configurations in `ensemble_model.py`:

```python
def build_xgboost(self):
    return xgb.XGBRegressor(
        n_estimators=500,      # Increase trees
        max_depth=8,           # Deeper trees
        learning_rate=0.05,    # Slower learning
        # ... other parameters
    )
```

---

## ğŸ“ˆ Results & Insights

### Top Influential Features

Based on SHAP analysis, the most important factors are typically:

1. **Hours Studied** - Direct correlation with performance
2. **Attendance** - Consistent class participation matters
3. **Previous Scores** - Historical performance is predictive
4. **Tutoring Sessions** - Additional support helps
5. **Sleep Hours** - Rest impacts cognitive function

### Model Insights

- **XGBoost** typically performs best on this tabular data
- **Neural Network** captures complex non-linear patterns
- **Random Forest** provides robust predictions
- **Ensemble** combines their strengths for optimal results

---

## ğŸ¤ Contributing

This is an academic project, but suggestions are welcome:

1. Fork the repository
2. Create a feature branch
3. Make your improvements
4. Submit a pull request

---

## ğŸ“„ License

This project is created for educational purposes as part of an ML course final project.

---

## ğŸ™ Acknowledgments

- Dataset: Student Performance Factors
- SHAP Library: For explainability
- Streamlit: For rapid dashboard development
- scikit-learn, XGBoost, TensorFlow: ML frameworks

---

## ğŸ“§ Contact

For questions about this project, please contact through your course channels.

---

**â­ If you found this project helpful, please star it!**
